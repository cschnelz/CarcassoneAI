{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "from tqdm import tqdm\n",
    "from Game import Game\n",
    "from State import State\n",
    "from Action import Action\n",
    "from typing import List, Dict, TYPE_CHECKING, Any\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "import queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class Agent(ABC):\n",
    "    @abstractmethod\n",
    "    def getResponse(self, validActions, game=None, maxPlayer=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Saver_Node:\n",
    "\n",
    "    ## Node for MCTS_saver, uses action history to recreate states rather than storing each one\n",
    "    def __init__(self, action:Action, parent:'Saver_Node', maxPlayer: int):\n",
    "        self.zero_wins = 0\n",
    "        self.one_wins = 0\n",
    "        self.visits = 0\n",
    "\n",
    "        if parent is None:\n",
    "            self.num_parents = 0\n",
    "        else:\n",
    "            self.num_parents = parent.num_parents + 1\n",
    "\n",
    "        self.terminal = False\n",
    "\n",
    "        #self.state = state ## The state we are in\n",
    "        self.action = action ## The action that got us here\n",
    "        self.parent = parent ## The parent node we are descendent from\n",
    "\n",
    "        ## will be a dict of {action : childNode } where childnode state is the result of applying action to this node state\n",
    "        self.children: Dict['Action','Saver_Node'] = {}\n",
    "        self.player = maxPlayer # needs to be changed to \"zero\" \"one\" and \"random\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self.parent is None:\n",
    "            p = \"None\"\n",
    "        else:\n",
    "            p = \"TODO: Hash\"\n",
    "            ## p = get_hash(self.parent.state)\n",
    "        try:\n",
    "            expected_value = self.get_expected_value()\n",
    "        except ValueError:\n",
    "            expected_value = 0\n",
    "        spacer = \"   \" * self.num_parents\n",
    "        return spacer + f'Action {self.action}\\n' + spacer + f' Visits={self.visits} Zero Wins={self.zero_wins} One Wins={self.one_wins}\\n' + spacer + f' Expected Value={expected_value} UCB={self.get_ucb()}'\n",
    "\n",
    "    ## given a mutable rootState, reconstructs the state associated with this node\n",
    "    def construct_state(self, rootState: State) -> State:\n",
    "        action_hist = []\n",
    "        curr = self\n",
    "        while curr.parent is not None:\n",
    "            action_hist.insert(0,curr.action)\n",
    "            curr = curr.parent\n",
    "        \n",
    "        [rootState.applyAction(action) for action in action_hist]\n",
    "        return rootState\n",
    "\n",
    "    ## MAKE SURE TO FLIP PLAYER BEFORE CALLING\n",
    "    def add_child(self, action:Action, player:int) -> 'Saver_Node':\n",
    "        if action in self.children.keys():\n",
    "            raise ValueError('dupe child')\n",
    "        else:\n",
    "            self.children[action] = Saver_Node(action, self, player)\n",
    "            return self.children[action]\n",
    "\n",
    "    def get_p_win(self, player:int):\n",
    "        try:\n",
    "            if player == 0:\n",
    "                return self.zero_wins / self.visits\n",
    "            elif player == 1:\n",
    "                return self.one_wins / self.visits\n",
    "            else:\n",
    "                raise ValueError(f'Given {player} for player, need 1 or 0')\n",
    "        except ZeroDivisionError:\n",
    "            raise ValueError('need atleast one visit before getting pwin')\n",
    "\n",
    "    def get_expected_value(self) -> float:\n",
    "        try:\n",
    "            return (self.zero_wins - self.one_wins) / self.visits\n",
    "        except ZeroDivisionError:\n",
    "            raise ValueError('need atleast one visit before getting expected value')\n",
    "\n",
    "    def get_explore_term(self, parent:'Saver_Node', c=1) -> float:\n",
    "        if self.parent is not None:\n",
    "            return c * (2*math.log(parent.visits) / self.visits) ** (1/2)\n",
    "        return 0\n",
    "    \n",
    "    def get_ucb(self, c=1, default=6) -> float:\n",
    "        if self.visits:\n",
    "            p_win = self.get_expected_value()\n",
    "            if self.player == 0:\n",
    "                p_win *= -1\n",
    "            explore_term = self.get_explore_term(self.parent,c)\n",
    "            return p_win + explore_term\n",
    "        return default\n",
    "\n",
    "    def print_tree(self, max_nodes = 50):\n",
    "        if max_nodes is None:\n",
    "            max_nodes = len(self.children)+1\n",
    "        print(f\"Printing from node TODO: Hash {self}\")\n",
    "        q = queue.Queue()\n",
    "        q.put(self)\n",
    "        node_count = 0\n",
    "        while not q.empty() and node_count < max_nodes:\n",
    "            node_count += 1\n",
    "            n = q.get()\n",
    "            print(n)\n",
    "            print()\n",
    "            for key in n.children.keys():\n",
    "                q.put(n.children[key])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(order=list(range(0,72)))\n",
    "root = Saver_Node(None,None,0)\n",
    "\n",
    "a1 = game.state.currentActions[0]\n",
    "c1 = root.add_child(a1,1)\n",
    "\n",
    "mState = game.startSim()\n",
    "reconstructed_state = c1.construct_state(mState)\n",
    "input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_Saver(Agent):\n",
    "    \n",
    "    def getLegalMoves(state: State) -> List[Action]:\n",
    "        return state.getActions()\n",
    "\n",
    "    def nextPlayer(currentPlayer:int) -> int:\n",
    "        return (currentPlayer + 1) % 2\n",
    "\n",
    "    def getResult(state: State) -> int:\n",
    "        if state.gameOver() is False:\n",
    "            return None\n",
    "        return state.scoreDelta()\n",
    "\n",
    "    ####*******####\n",
    "    #             #  \n",
    "    # ENTRY POINT #\n",
    "    #             #\n",
    "    ####*******####\n",
    "\n",
    "    def getResponse(self, validActions: List[Action], game:Game=None, maxPlayer:int=None) -> Action:\n",
    "\n",
    "        \n",
    "        self.headState = game.state      ## <- a reference of the root state for rollback\n",
    "        self.muteState = game.startSim() ## <- make a deepcopy of the headstate that we can play around with\n",
    "\n",
    "        root = Saver_Node(None,None,maxPlayer)\n",
    "        for iteration in range(60):\n",
    "            v = MCTS_Saver.tree_policy(root, maxPlayer, self.muteState)\n",
    "            game.refresh(self.muteState) ## refresh the state to where it was before TP\n",
    "            score = MCTS_Saver.default_policy(v, game, self.muteState)\n",
    "            game.refresh(self.muteState)\n",
    "            MCTS_Saver.backProp(v, score)\n",
    "        move = MCTS_Saver.bestChild(root, 0)\n",
    "\n",
    "        root.print_tree()\n",
    "        print(f\"\\n\\nBEST NODE: {move.action} UCB: {move.get_ucb()}\")\n",
    "\n",
    "        return move.action\n",
    "\n",
    "\n",
    "    ####*******####\n",
    "    #             #  \n",
    "    # POLICYFUNCS #\n",
    "    #             #\n",
    "    ####*******####\n",
    "\n",
    "    ## MCTS tree policy (selects child node to examine)\n",
    "    def tree_policy(node: Saver_Node, player:int, muteState: State) -> Saver_Node:\n",
    "        ## return the node if its a terminal\n",
    "        if node.terminal:\n",
    "            return node \n",
    "        \n",
    "        ## otherwise expand a new possible childNode <--- This is where the random will come in but for now its deterministic\n",
    "        if node.action is not None:\n",
    "            muteState.applyAction(node.action)\n",
    "        moves = MCTS_Saver.getLegalMoves(muteState)\n",
    "        if len(moves) > len(node.children):\n",
    "            return MCTS_Saver.expand(node, player, moves)\n",
    "        \n",
    "        ## if all children have been expanded, go down the tree by what we think is the best candidate and recurs\n",
    "        return MCTS_Saver.tree_policy(MCTS_Saver.bestChild(node), MCTS_Saver.nextPlayer(player), muteState)\n",
    "\n",
    "    ## Adds a random successor node\n",
    "    def expand(node:Saver_Node, player:int, actions: List[Action]) -> Saver_Node:\n",
    "        child = None\n",
    "        for action in actions:\n",
    "            if action not in node.children.keys():\n",
    "                child = node.add_child(action,MCTS_Saver.nextPlayer(player))\n",
    "                return child\n",
    "        raise ValueError(\"Ran out of children when we shouldn't\")\n",
    "\n",
    "    ## Selection heuristic for following tree and finally move choice\n",
    "    def bestChild(node:Saver_Node, c=1) -> Saver_Node:\n",
    "        bestNode = list(node.children.values())[0]\n",
    "        for action, node in node.children.items():\n",
    "            if node.get_ucb(c) > bestNode.get_ucb(c):\n",
    "                bestNode = node\n",
    "        return bestNode\n",
    "\n",
    "    ## Recurse up the tree now, incrementing vists and accumulating score\n",
    "    def backProp(node:Saver_Node,score:int) -> None:\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            if score > 0:\n",
    "                node.zero_wins += score\n",
    "            elif score < 0:\n",
    "                node.one_wins += score\n",
    "            node = node.parent\n",
    "\n",
    "    ## Rollout randomly from a gamestate to game end\n",
    "    def default_policy(node:Saver_Node,game:Game, muteState: State, print_final=False) -> int:\n",
    "        current_state = node.construct_state(muteState)\n",
    "        \n",
    "        while(current_state.gameOver() is False):\n",
    "            moves = MCTS_Saver.getLegalMoves(current_state)\n",
    "            current_state.applyAction(random.choice(moves), quiet=True)\n",
    "        return MCTS_Saver.getResult(current_state)\n",
    "\n",
    "######################\n",
    "#  Save a head state copy for the mcts search\n",
    "#  when you get to default policy instead of copying the current node's state\n",
    "#  create a new state by applying actions from the original state\n",
    "#  when done, shallow-copy-restore the head state copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing from node TODO: Hash Action None\n",
      " Visits=60 Zero Wins=119 One Wins=-14\n",
      " Expected Value=2.216666666666667 UCB=-2.216666666666667\n",
      "Action None\n",
      " Visits=60 Zero Wins=119 One Wins=-14\n",
      " Expected Value=2.216666666666667 UCB=-2.216666666666667\n",
      "\n",
      "   Action Location: [-1, 0] Orientation: 0 \n",
      "    Visits=6 Zero Wins=0 One Wins=-14\n",
      "    Expected Value=2.3333333333333335 UCB=3.501571973655036\n",
      "\n",
      "   Action Location: [-1, 0] Orientation: 0 Meeple: Feature: CITY on edges [2]\n",
      "    Visits=2 Zero Wins=2 One Wins=0\n",
      "    Expected Value=1.0 UCB=3.023448680402372\n",
      "\n",
      "   Action Location: [-1, 0] Orientation: 0 Meeple: Feature: GRASS on edges [0, 1, 6, 7]\n",
      "    Visits=1 Zero Wins=0 One Wins=0\n",
      "    Expected Value=0.0 UCB=2.8615885665909766\n",
      "\n",
      "   Action Location: [-1, 0] Orientation: 0 Meeple: Feature: CITY on edges [1]\n",
      "    Visits=8 Zero Wins=22 One Wins=0\n",
      "    Expected Value=2.75 UCB=3.761724340201186\n",
      "\n",
      "   Action Location: [-1, 0] Orientation: 3 \n",
      "    Visits=1 Zero Wins=0 One Wins=0\n",
      "    Expected Value=0.0 UCB=2.8615885665909766\n",
      "\n",
      "   Action Location: [-1, 0] Orientation: 3 Meeple: Feature: GRASS on edges [6, 7, 4, 5]\n",
      "    Visits=1 Zero Wins=0 One Wins=0\n",
      "    Expected Value=0.0 UCB=2.8615885665909766\n",
      "\n",
      "   Action Location: [-1, 0] Orientation: 3 Meeple: Feature: CITY on edges [1]\n",
      "    Visits=7 Zero Wins=18 One Wins=0\n",
      "    Expected Value=2.5714285714285716 UCB=3.65300738596936\n",
      "\n",
      "   Action Location: [-1, 0] Orientation: 3 Meeple: Feature: CITY on edges [0]\n",
      "    Visits=2 Zero Wins=2 One Wins=0\n",
      "    Expected Value=1.0 UCB=3.023448680402372\n",
      "\n",
      "   Action Location: [0, -1] Orientation: 0 \n",
      "    Visits=1 Zero Wins=0 One Wins=0\n",
      "    Expected Value=0.0 UCB=2.8615885665909766\n",
      "\n",
      "   Action Location: [0, -1] Orientation: 0 Meeple: Feature: CITY on edges [2]\n",
      "    Visits=18 Zero Wins=56 One Wins=0\n",
      "    Expected Value=3.111111111111111 UCB=3.7855940045785683\n",
      "\n",
      "   Action Location: [0, -1] Orientation: 0 Meeple: Feature: CITY on edges [1]\n",
      "    Visits=2 Zero Wins=2 One Wins=0\n",
      "    Expected Value=1.0 UCB=3.023448680402372\n",
      "\n",
      "   Action Location: [0, -1] Orientation: 0 Meeple: Feature: GRASS on edges [0, 1, 6, 7]\n",
      "    Visits=1 Zero Wins=0 One Wins=0\n",
      "    Expected Value=0.0 UCB=2.8615885665909766\n",
      "\n",
      "   Action Location: [0, -1] Orientation: 1 \n",
      "    Visits=1 Zero Wins=0 One Wins=0\n",
      "    Expected Value=0.0 UCB=2.8615885665909766\n",
      "\n",
      "   Action Location: [0, -1] Orientation: 1 Meeple: Feature: CITY on edges [3]\n",
      "    Visits=2 Zero Wins=3 One Wins=0\n",
      "    Expected Value=1.5 UCB=3.523448680402372\n",
      "\n",
      "   Action Location: [0, -1] Orientation: 1 Meeple: Feature: GRASS on edges [2, 3, 0, 1]\n",
      "    Visits=1 Zero Wins=0 One Wins=0\n",
      "    Expected Value=0.0 UCB=2.8615885665909766\n",
      "\n",
      "   Action Location: [0, -1] Orientation: 1 Meeple: Feature: CITY on edges [2]\n",
      "    Visits=6 Zero Wins=14 One Wins=0\n",
      "    Expected Value=2.3333333333333335 UCB=3.501571973655036\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 \n",
      "       Visits=1 Zero Wins=0 One Wins=0\n",
      "       Expected Value=0.0 UCB=1.8930184728248454\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 Meeple: Feature: GRASS on edges [2, 3]\n",
      "       Visits=1 Zero Wins=0 One Wins=0\n",
      "       Expected Value=0.0 UCB=1.8930184728248454\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 Meeple: Feature: CITY on edges [3, 0, 2]\n",
      "       Visits=1 Zero Wins=0 One Wins=-1\n",
      "       Expected Value=1.0 UCB=0.8930184728248454\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 0 \n",
      "       Visits=1 Zero Wins=0 One Wins=0\n",
      "       Expected Value=0.0 UCB=1.8930184728248454\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 0 Meeple: Feature: GRASS on edges [4, 5]\n",
      "       Visits=1 Zero Wins=0 One Wins=0\n",
      "       Expected Value=0.0 UCB=1.8930184728248454\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 \n",
      "       Visits=1 Zero Wins=1 One Wins=0\n",
      "       Expected Value=1.0 UCB=0.17741002251547466\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 \n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-0.9606660196623822\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 Meeple: Feature: GRASS on edges [2, 3]\n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-0.9606660196623822\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 Meeple: Feature: CITY on edges [3, 0, 2]\n",
      "       Visits=1 Zero Wins=2 One Wins=0\n",
      "       Expected Value=2.0 UCB=0.03933398033761781\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 0 \n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-0.9606660196623822\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 0 Meeple: Feature: GRASS on edges [4, 5]\n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-0.9606660196623822\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 0 Meeple: Feature: CITY on edges [0, 1, 3]\n",
      "       Visits=1 Zero Wins=2 One Wins=0\n",
      "       Expected Value=2.0 UCB=0.03933398033761781\n",
      "\n",
      "      Action Location: [-1, 1] Orientation: 0 \n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-0.9606660196623822\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 \n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-1.0272302977512489\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 Meeple: Feature: GRASS on edges [2, 3]\n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-1.0272302977512489\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 Meeple: Feature: CITY on edges [3, 0, 2]\n",
      "       Visits=1 Zero Wins=2 One Wins=0\n",
      "       Expected Value=2.0 UCB=-0.027230297751248855\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 1 \n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-1.0272302977512489\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 1 Meeple: Feature: GRASS on edges [6, 7]\n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-1.0272302977512489\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 1 Meeple: Feature: CITY on edges [1, 2, 0]\n",
      "       Visits=1 Zero Wins=1 One Wins=0\n",
      "       Expected Value=1.0 UCB=0.9727697022487511\n",
      "\n",
      "      Action Location: [-2, 0] Orientation: 3 \n",
      "       Visits=1 Zero Wins=1 One Wins=0\n",
      "       Expected Value=1.0 UCB=0.17741002251547466\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 3 \n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-0.5956823180385813\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 3 Meeple: Feature: CITY on edges [3, 0, 2]\n",
      "       Visits=1 Zero Wins=2 One Wins=0\n",
      "       Expected Value=2.0 UCB=0.4043176819614187\n",
      "\n",
      "      Action Location: [-1, -1] Orientation: 3 Meeple: Feature: GRASS on edges [2, 3]\n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-0.5956823180385813\n",
      "\n",
      "      Action Location: [-1, 0] Orientation: 0 \n",
      "       Visits=1 Zero Wins=4 One Wins=0\n",
      "       Expected Value=4.0 UCB=-1.5956823180385813\n",
      "\n",
      "      Action Location: [-1, 0] Orientation: 0 Meeple: Feature: GRASS on edges [4, 5]\n",
      "       Visits=1 Zero Wins=4 One Wins=0\n",
      "       Expected Value=4.0 UCB=-1.5956823180385813\n",
      "\n",
      "      Action Location: [-1, 0] Orientation: 1 \n",
      "       Visits=1 Zero Wins=4 One Wins=0\n",
      "       Expected Value=4.0 UCB=-1.5956823180385813\n",
      "\n",
      "      Action Location: [-1, 0] Orientation: 1 Meeple: Feature: GRASS on edges [6, 7]\n",
      "       Visits=1 Zero Wins=4 One Wins=0\n",
      "       Expected Value=4.0 UCB=-1.5956823180385813\n",
      "\n",
      "      Action Location: [-1, 0] Orientation: 2 \n",
      "       Visits=1 Zero Wins=4 One Wins=0\n",
      "       Expected Value=4.0 UCB=-1.5956823180385813\n",
      "\n",
      "      Action Location: [-1, 0] Orientation: 2 Meeple: Feature: GRASS on edges [0, 1]\n",
      "       Visits=1 Zero Wins=4 One Wins=0\n",
      "       Expected Value=4.0 UCB=-1.5956823180385813\n",
      "\n",
      "      Action Location: [0, -2] Orientation: 0 \n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-0.5956823180385813\n",
      "\n",
      "      Action Location: [0, -2] Orientation: 0 Meeple: Feature: CITY on edges [0, 1, 3]\n",
      "       Visits=1 Zero Wins=2 One Wins=0\n",
      "       Expected Value=2.0 UCB=0.4043176819614187\n",
      "\n",
      "      Action Location: [0, -2] Orientation: 0 Meeple: Feature: GRASS on edges [4, 5]\n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-0.5956823180385813\n",
      "\n",
      "      Action Location: [1, -1] Orientation: 0 \n",
      "       Visits=1 Zero Wins=3 One Wins=0\n",
      "       Expected Value=3.0 UCB=-0.5956823180385813\n",
      "\n",
      "\n",
      "\n",
      "BEST NODE: Location: [0, -1] Orientation: 0 Meeple: Feature: CITY on edges [2] UCB: 3.7855940045785683\n"
     ]
    }
   ],
   "source": [
    "def launch():\n",
    "    players = [MCTS_Saver(), MCTS_Saver()]\n",
    "    carcassonne = Game(players,order=list(range(0,72)))\n",
    "\n",
    "    actions = carcassonne.getActions()\n",
    "    currPlayer = carcassonne.currentPlayer()\n",
    "\n",
    "    response = currPlayer.agent.getResponse(actions,game=carcassonne,maxPlayer=currPlayer.id)\n",
    "    carcassonne.applyAction(response)\n",
    "\n",
    "    carcassonne.render()\n",
    "\n",
    "launch()\n",
    "# import cProfile, pstats\n",
    "# profiler = cProfile.Profile()\n",
    "# profiler.enable()\n",
    "# launch()\n",
    "# profiler.disable()\n",
    "# stats = pstats.Stats(profiler).sort_stats('tottime')\n",
    "# stats.print_stats()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
